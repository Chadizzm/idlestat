
================================================================================
.:: Tracing API [1]
================================================================================
Event of interest: cpu_idle and cpu_frequency
Format: state=%lu cpu_id=%lu

NOTE1: state = -1 | 4294967295' represents an EXIT from the current state

Example trace
-------------
This is an actual sequence of events generated on CPU3 (little)
Beginning labels and empty lines added just for readability


A:        <idle>-0     [003] d...  1469.842796: cpu_idle: state=1 cpu_id=3
          <idle>-0     [003] ....  1469.850945: cpu_idle: state=4294967295 cpu_id=3

B:        <idle>-0     [003] d...  1469.851036: cpu_idle: state=1 cpu_id=3


C:   kworker/0:2-2641  [000] ....  1469.963151: cpu_frequency: state=500000 cpu_id=3
          <idle>-0     [003] ....  1469.963435: cpu_idle: state=4294967295 cpu_id=3

D:        <idle>-0     [003] d...  1469.963461: cpu_idle: state=1 cpu_id=3
          <idle>-0     [003] ....  1470.022701: cpu_idle: state=4294967295 cpu_id=3

we can see these states transitions
1. enter WFI in A and stay on that state for (1469.850945-1469.842796)   8.149 [ms]
2. running in (undefined) P-State for (1469.851036-1469.850945)          0.091 [ms]
3. enter WFI in B and stay on that state for (1469.963435-1469.851036) 112.399 [ms]
4. running in 500MHz P-State for (1469.963461-1469.963435)               0.026 [ms]
5. enter WFI in D abd stay on that state for (1470.022701-1469.963461)  59.240 [ms]

NOTE2: the cpu_idle events are generated by the cpuidle_states::enter function
defined by the big.LITTLE CPUIdle driver [2]. Such a function in turns traps
into the cpu_suspend routine provided by the PSCI interface.
=> Thus, cluster power down, once all its CPUs are in C1 mode, is expected to be
   managed by the PSCI firmware and not explicitely reported by an FTrace event.

NOTE3: we can assume that the PSCI firmware will:
1. put the cluster down: when _all_ the CPUs of a cluster have entered C1
2. put the cluster up: whe _at least_ one CPU of the cluster exited C1
=> The number of times a cluster hits the C1 state is expected to be _not
   greater_ than the minimum number of hits on that state for the CPUs
   belonging to that cluster.

========================
.:: State initialization
========================

The trace starts with a set of WFI enter events, which are properly forced by the
idlestat_wake_all function. That function force a wakeup on all cores by simply
setting the CPU affinitiy on each core in turn.

Example
  <idle>-0     [001] d...  1469.085314: cpu_idle: state=1 cpu_id=1
  <idle>-0     [004] ....  1469.086479: cpu_idle: state=4294967295 cpu_id=4
  <idle>-0     [000] .n..  1469.086516: cpu_idle: state=4294967295 cpu_id=0
  <idle>-0     [003] .n..  1469.086521: cpu_idle: state=4294967295 cpu_id=3
  <idle>-0     [004] d...  1469.086542: cpu_idle: state=1 cpu_id=4
  <idle>-0     [003] d...  1469.086718: cpu_idle: state=1 cpu_id=3
  <idle>-0     [000] d...  1469.086728: cpu_idle: state=1 cpu_id=0
  <idle>-0     [001] ....  1469.088010: cpu_idle: state=4294967295 cpu_id=1
  <idle>-0     [002] .n..  1469.088041: cpu_idle: state=4294967295 cpu_id=2
  <idle>-0     [001] d...  1469.088048: cpu_idle: state=1 cpu_id=1
  <idle>-0     [002] d...  1469.088217: cpu_idle: state=1 cpu_id=2

NOTE4: whan the core wake up we still don't know the P-State in which it is running.
If a core was already up and running, we miss both the WFI exit event as well as we
don't know the current frequency.
Should we try to force a CPUfreq event, e.g. by setting the 'performance' governor
and than switch back to 'ondemand'?


================================================================================
.:: Idlestat Analysis
================================================================================

1. 'cpuidle_cstates' (CS)  and 'cpufreq_pstates' (PS) data structures are created and
initialized while the trace is parsed, according to 'cpu_idle' and 'cpu_frequency'
events found.
Such structures are headed by the 'cpuidle_datas' (CD) main structure.

2. CS and PS data structures are "mapped" into the tepology after the trace has
been parsed, and the topology information recovered by the trace head itself.
The topology tree (TT) has just pointers to C and P state structures.
The function: 'establish_idledata_to_topo' is in charge to bind 'cpuidle_datas'
to the pointers exposed byt the TT.

3. in the case of the big.LITTLE platform:
a) each "CPU point" of the TT is binded with the specific CS and PS data for that CPU
b) since each core has just one cpu, each "CORE point" of the TT is linked with _the
   same_ CS and PS data for the corresponding CPU.
   NOTE5: this mapping is different just in case of cores with HT cpus.
c) each "PHYSICAL point" of the TT is mapped to a _newly allocated_ CS data
   structure, which is than filled up according to a certain elaboration of CS
   data from the cores, i.e. from the cpus (ref. point b), that belong to that
   physical.

PHYSICAL Points setup
=====================

Each "Physical Point" (PP) is filled by processing CState info of the Cores,
i.e. CPUs, belonging to it:

1. the maximum CState used by its CPUs is identified in order to "optimze" the
   following steps.
2. intersections among _same_ C-States of different Cores are identified by
   comparing the C-States permanency intervals of one CPU with those on the
   others in the same PP.
   NOTE6: a new C-State hit data is created for the PP per each non-empty
   intersection that could be found.

Here it is a simple example, let's consider that simple "hand crafted" trace:

Example trace 1 (only C-States hits on big cluster)
---------------------------------------------------
  <idle>-0     [001] d...  0.000000: cpu_idle: state=0 cpu_id=1
  <idle>-0     [002] d...  0.000000: cpu_idle: state=0 cpu_id=2
  <idle>-0     [001] d...  0.000100: cpu_idle: state=4294967295 cpu_id=1
  <idle>-0     [002] d...  0.000200: cpu_idle: state=4294967295 cpu_id=2
  <idle>-0     [001] d...  0.000110: cpu_idle: state=0 cpu_id=1
  <idle>-0     [002] d...  0.000210: cpu_idle: state=1 cpu_id=2
  <idle>-0     [001] d...  0.000320: cpu_idle: state=4294967295 cpu_id=1
  <idle>-0     [002] d...  0.000400: cpu_idle: state=4294967295 cpu_id=2
  <idle>-0     [001] d...  0.000350: cpu_idle: state=1 cpu_id=1
  <idle>-0     [001] d...  0.000400: cpu_idle: state=4294967295 cpu_id=1
  <idle>-0     [002] d...  0.000410: cpu_idle: state=0 cpu_id=2
  <idle>-0     [002] d...  0.000500: cpu_idle: state=4294967295 cpu_id=2

Which corresponds to these C-States hits:

Trace hits (just C-States for the big cluster)
----------------------------------------------
root@linaro-developer:/data/local# ./traceParse.sh trace_test.dat
Total CPUFreq events: 0
Total CPUIdle events: 12

.:: big (ClusterA)
I       0 hits: CPU01:     2, CPU02:     2
I       1 hits: CPU01:     1, CPU02:     1

When parsed by idlestat, it is able to identify these intersections:

Idlestat intersections
----------------------
Intersect [WFI: 0.000000-0.000100] with [WFI: 0.000000-0.000200] => 0.000000-0.000100 (+100.000000)
Intersect [WFI: 0.000110-0.000320] with [WFI: 0.000000-0.000200] => 0.000110-0.000200 (+90.000000)
Intersect [C1: 0.000350-0.000400] with [C1: 0.000210-0.000400] => 0.000350-0.000400 (+50.000000)

Which generates that report:

Idlestat report
---------------
clusterA@state	hits	      total(us)		avg(us)	min(us)	max(us)
         WFI	2	         190.00	          95.00	0.00	100.00
         C1	1	          50.00	          50.00	0.00	50.00
    cpu1@state	hits	      total(us)		avg(us)	min(us)	max(us)
         WFI	2	         310.00	         155.00	100.00	210.00
         C1	1	          50.00	          50.00	50.00	50.00
    cpu2@state	hits	      total(us)		avg(us)	min(us)	max(us)
         WFI	2	         290.00	         145.00	90.00	200.00
         C1	1	         190.00	         190.00	190.00	190.00


Cluster level status analysis
=============================

The intersection algorithm implemented by idlestat is capable to identify
the overlapping of a similar C-States among different CPUs of the same cluster.

NOTE7:
------
In case the two CPUs are in different C-States at the same time, no
intersections are identified.
Should not we consider that cluster in the minimum C-State (i.e. more power
consuming) among all the ones available?


Here is another hand crafted trace to point out the scenario.
Let's consider these sequence of C-States for the two CPUs of the big cluster.
The second plot report the intersections identified by idlestat, while the
third one the intersections that I would probably expectd.

         +--------------+--------------+--------------+
    CPU1 |      C0      |      C1      |      Px      |
         +----+----+---------+----+---------+----+----+
    CPU2 | C0 | C1 | Px | C0 | C1 | Px | C0 | C1 | Px |   Time
         +----+----+----+----+----+----+----+----+----+-------->

         +----+              +----+
ClusterA | C0 |              | C1 |    <= Idlestat intersections
         +----+              +----+

         +---------+    +---------+
ClusterA | C0   C0 |    | C0   C1 |    <= Expected intersections
         +---------+    +---------+

For completeness of the example, here are the energy figures reporetd by
idlestat for this execution scenario:

Idlestat results
----------------
Intersect [WFI: 0.000000-0.000300] with [WFI: 0.000000-0.000100] => 0.000000-0.000100 (+100.000000)
Intersect [C1: 0.000300-0.000600] with [C1: 0.000400-0.000500] => 0.000400-0.000500 (+100.000000)
Compute idle data intersections...
clusterA@state	hits	      total(us)		avg(us)	min(us)	max(us)
         WFI	1	         100.00	         100.00	0.00	100.00
         C1	1	         100.00	         100.00	0.00	100.00
    cpu1@state	hits	      total(us)		avg(us)	min(us)	max(us)
         WFI	1	         300.00	         300.00	300.00	300.00
         C1	1	         300.00	         300.00	300.00	300.00
    cpu2@state	hits	      total(us)		avg(us)	min(us)	max(us)
         WFI	3	         300.00	         100.00	100.00	100.00
         C1	3	         300.00	         100.00	100.00	100.00

energy consumption from cap states 	0.000000e+00 (A)
energy consumption from idle 		2.500000e+03
energy consumption from wakeups 	2.150400e+05
total energy consumption estimate 	2.175400e+05


(A) is 0 since this example does not expose cpu_frequency events


Energy Model Computation
========================

Here are some notes on the current energy mode computation which comes out from
the review of the 'calculate_energy_consumptions' procedure:

Idle Energy - Cluster Level
---------------------------

A contribution is added for _each_ state repored by CPUIdle.
Thus, not only the C1 state is considered, but the WFI it is as well.
From the previous example:

clusterA@state	hits	      total(us)		avg(us)	min(us)	max(us)
         WFI	1	         100.00	         100.00	0.00	100.00
         C1	1	         100.00	         100.00	0.00	100.00

Thus, two contributions are going to be accounted.

NOTE8: The energy mode expose an idle_power_cluster only for the WFI C-State,
while the C1 state is not energg characterized.

Accrding to the model:
a) the idle_power_cluster[C1] should be multiplied by C1 permanency
b) the idle_power_cluster[WFI] is NOT defined

Idle Energy - CPUs
------------------

A contribution is added for _each_ CPU C-State, however the implementation
_subtract_ from _all_ CStates residency time the time in which the cluster has
been profiled to be in C1 state. These seems not a proper implementation of the
model, which is:

(WFI_residency + C1_residency_with_cluster_on) * idle_power_cpu

what the implementation does for that contribution seems to be:

(WFI_residency - C1_cluster) * idle_power_cpu


Cap Energy - CPUs
-----------------

A contribution is added for each P-State of each CPU.

The only observation in that regard is that the trace is not properly forged to
know what is exactly the CPUFreq selected frequency at the beginning of the
test. Thus, for example, if a test starts with a task on a CPU which is already
active and running at full speed, than all the energy consumed up to the next
CPUFreq scaling event is not properly accounted.
=> a CPUFreq transition, on each CPU, should probably be forced at the
beginning of each test, e.g. by forcing a governor switch?

Moreover, the maximum residency within each C-State, among the CPUs of a
cluster, is tracked to be used as a proxy for the Cluster Cap Energy
computation (see next point).


Cap Energy - Cluster
--------------------

The P-States duration for clusters are _not_ currently computed.
Thus, a proxy is computed as:

  cluster[CS].duration = MAX(cpu[CS].duration) ... for each CPU in that cluster

This per-CS duration is that multiplied by the cluster power in that CS.


Wakeup Energy - CPUs
--------------------

It is currently _not_ computed

Since it has almost the same "structure" of the Idle Energy for the CPUs, I
would expect the same issues/wakeness of that previous computation as well.

Wakeup Energy - Cluster
-----------------------

A contribution is added for _each_ state reported by CPUIdle.
Thus, WFI as well as C1 stats hits are summed up and multiplied by the
model energy parameter (wakeup_energy_cluster)

According to the mode, just the hits to C1 should be considered.
While, the WFI events should be probably related to the wakeup_energy_cpu
parameter... to be better discussed.



Specifications for Energy Model Computation
===========================================

The Energy Model for a TC2 system has been designed to be generic with respect
to a generic Heterogeneous system which could have frequency and voltage
domains grouping multiple CPUs. A TC2 system has indeed these P and C state
domains:
  ClusterA: 2x A15
  ClusterB: 3x  A7

This architecture enforces these constraints:
1. frequencies could be configured only at cluster level, thus:
   *all* the CPUs in a cluster, if not idle, are running on the *highest*
   P-State required by the CPUs of that cluster.
2. idle states could be configured only at cluster level, thus:
   *all* the CPUs in a cluster, if not running, are placed at minimum on
   *highest* C-State entered by the CPUs of that cluster.

==========================================
.:: Weakeness for Energy Model Computation
==========================================

Based on our analysis this is the list of main wake point which makes idlestat,
in its current implementation, a tool not suitable for a corrent computation of
the proposed Energy Model:

1. Uncertainty of initial C-States
   The current implementation attempts to enforce a C-State transition by
   pinning the main thread on all CPUs. This CPUs visit is however not granting
   a C-State enter for example in all cases when the target system is completely
   idle but one of the visited CPUs which is already running some workloads;

2. Uncertainty of initial P-States
   The current implementation does not provide any kind of support to know what
   is the initial working frequency of each CPUs. The poke routine described in
   the previous point is not granting any CPUFreq transition since the pinned
   workload is almost negligible and last for really few [us];

3. Undefined amount of CPUFreq and CPUIdle events before the start of the actual
   workload under analysis.
   The number of events generated by the system before and after the execution
   of a workload is not known and it depends on the actual load of the system.
   Even if these events could be considered to have a negligible impact on the
   overall energy consumption due to the workload execution, a better
   identification of the execution related events could improve the statistical
   figures of repeated experiments;

4. Power/Frequency domains uncorrelated CPUFreq and CPUIdle events
   The CPUfreq and CPUIdle frameworks generate per-CPU events which do not
   keep under consideration the target system topology related to Power and
   Frequency domains, which is a main limitation for a proper exploitation of
   the tool on architectures such as a TC2.
   For example, if one of the big CPUs in ClusterA is running at 500MHz and the
   second one starts the execution of a new workload at 1GHz, the CPUFreq
   framework:
     a) generate 1 event reporting 1GHz for CPU2
     b) does not generate any event reporting that also CPU1 is now at 1GHz

   Even worst is the case of idle state entrance since the CPUIdle framework is
   not performing any kind of per-cluster "aggregation" of the idle state
   entrance requests. Thus, for example, a CPU could be reported to enter C1
   state just because that has been the CPUIdle request for that cpu, while all
   the cluster is kept at WFI because of another CPU asked for just that idle
   state.

5. Missing of cluster-level P-State residency information
   The current implementation does not keep track of per-CPU P-States entry and
   exit time, but just an overall residency duration within each state.
   This does not allows the tool to properly compute for how long the complete
   cluster was running on a P-State by considering the architectural contraints
   enforced at cluster level cluster.
   For example, the cluster is always running at the maximum P-State in used by
   one of its CPus.

6. Missing of set of validation traces
   A set of synthetic traces, specifically forged to represent corner-cases,
   could be a valuable support to test the tool and easy its validation.
   For example, by using pre-defined and human friendly events timestamps.

7. Missing of a support for "graphical" validation
   A graphical representation of state transitions could provide a
   valuable support to validate tools computed residency timing, their matching
   with the target topology and finally the energy estimation numbers computed
   by the tools.


Code related weaknesses
-----------------------

1. cpuidle_cstates and cpufreq_pstates are dissimilar data structures,
   despite they tracks quite similar information. The same is for
   cpuidle_cstate and cpufreq_pstate.

2. cpuidle_cstates is initialized using hard-coded CStates number, a suboptimal
   solution which however allows to import for analysis traces generated on a
   different target.

3. cpufreq_pstates is initialized based on sysfs exposed data, a more
   space-optimal solution which however does not allow to import for analysis a
   trace generated on a different target.

4. cpufreq_pstate does not track event entry and exit timestamp, thus not
   supporting a proper computation of Cluster-Level PState.


Possible solutions for the highlighted wakeness
===============================================

1 and 2.
   The FTrace marker interface (i.e. /sys/kernel/debug/tracing/trace_marker)
   could be exploited to inject "fake" CPUFreq events at the beginning of an
   execution to report the current working frequency for each CPU.
   Such working frequency could be read via sysfs:
   /sys/devices/system/cpu/cpuX/cpufreq/cpuinfo_cur_freq

3.
   The same FTrace marker interface could be exploited to inject in the trace a
   couple of idlestat specific start and end tracing markers.
   All the events reported before the start marker will be used only to identify
   the testing conditions, while the end marker could be used to define the
   end-of-time for the energy model computation.

   Reference implementation:
   git@pdsw-git:users/patbel01/idlestat  drk-em-validation


4.
   bit.LITTLE systems exploit the PSCI to properly coordinate P- and C-States
   transitions. This is just a sort of "state machine" which collects CPUFreq
   and CPUIdle requests, it aggregates them to figure out a Cluster-Level
   configuration and enforce a cluster P and C state which matches the requests
   of all the CPUs on that cluster.
   A simple yet effective solution could be obtained by "filtering" the ftrace
   output to generate a suitable set of events which reflects the actual
   configuration actions taken by the PSCI.

   Reference implementation:
   git@pdsw-git:users/patbel01/idlestat  drk-psci-proxy

   Status: filtering working, still to fix the integration with the energy model
           computation code which now returns null energy consumption because of
           some parsing issue of the new trace still to be fixed.

5.
   A simple solution could be to replicate the code used for CState
   entry and exit timestamps tracking for PStates residency tracking too.
   At the end of a trace parsing, the same code which is already in charge to
   identify Cluster-Level events for CStates residencies could be used to
   compute PStates related residencies times.

6.
   A simple dataset could be generated while filtering the ftrace output
   to report the PSCI status recovered by the per-CPUs events.

   Reference implementation:
   git@pdsw-git:users/patbel01/idlestat  drk-gnuplot



================================================================================
References
================================================================================
[1] Documentation/trace/events-power.txt
[2] drivers/cpuidle/cpuidle-big_little.c

Others Sources
==============
- Multi-cluster power management
  http://lwn.net/Articles/539082/
- Power Management Changes and Secure Firmware in Linaro 13.03 onwards
  http://wiki.arm.com/ASD/PSCIpowermanagementdriver

# vim: tw=80 fo=cqt wm=0
